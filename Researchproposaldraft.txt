# Text for My Research Proposal
## SubHeading

Genetics is a powerful tool in Ecology.
It is becoming affordable to perform high throughput genome sequencing (often with reduced representation of the genome) for ecological studies. Large amounts of genetic data are easily produced for species in which there is very little existing genetic data. There is generally no reference for sequences to be aligned to, although this will become less of an issue as more taxa have published genomes.

High throughput sequence analysis requires the use of high performance computing. Tools to create a trustworthy dataset from these sequencing runs are generally free, but require a much higher level of computing knowlegdge than is possessed by most biologists. The need for higher level knowledge of information technology has led to the burgeoning field of bioinformatics. Bioinformaticists are either computer scientists who have trained in basic genetic and biological knowledge in order to collaborate with biologists, or biologists who have made increasing their IT knowledge a priority. Computational tools for data processing and analysis are becoming more complex and abundant, and it is becoming increasingly common for research teams to have dedicated experts in bioinformatics to perform these tasks. It is becoming unfeasible for every researcher to learn how to effectively use all the computational tools needed to conduct an ecological genomics project.
Larger lab groups have been hiring dedicated bioinformatic experts, while smaller groups often rely on collaborating with researchers who have dedicated a large proportion of their career to solving bioinformatic issues. However, many researchers prefer to retain full control and understanding of the entire work flow in their project or do not have access to collaboration with bioinformaticians. These researchers generally find themselves spending a large proportion of their time learning ...

The bioinformatics problem is that expert ecologists and evolutionary biologists working with genomic data have to spend too much time becoming adept at complex bioinformatics

Most data processing programs are free, but require knowledge of UNIX command line to install and run. For the average biologist used to only using GUI applications, understanding and troubleshooting the use of these programs is a major task and uses up a large proportion of the time allocated for a research project.

There are many "wrapper scripts" that have been developed to allow biologists to follow a pre-defined bioinformatic workflow without requiring much knowledge of commandline interfaces. These are scripts that take raw data and some key parameters as input, then run the data through a predefined pipeline of programs before outputting a set of results. While there is generally some troubleshooting required just to install the programs called upon by the wrapper script, little knowledge or understanding of the processes are required to turn data into usable results. This attractively simple solution to the bioinformatics problem is useful where the data and project are a good fit for the analysis pipeline. As soon as the data doesn't conform to the assumptions of the wrapper, or the purpose of the project is different from the assumed requirements of the results, the wrapper can lead researchers to inappropriate or sub-optimal results.

A skilled bioinformatician knows that each dataset and each analysis can lead to unexpected results that require troubleshooting. A researcher relying on a wrapper script for their bioinforamtic data processing can unknowingly lose useful data, retain error and bias int he dataset, and produce a dataset that is
